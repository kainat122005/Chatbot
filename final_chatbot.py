# -*- coding: utf-8 -*-
"""Final chatbot

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f4LrN-Po6ncYRqmdedInUXW2fwa7JKV7
"""

pip install langchain --quiet

pip install streamlit qdrant-client langchain-qdrant HuggingFace Pypdf google_generativeai langchain_google_genai pyngrok  --quiet

pip install -U langchain-community --quiet

#loading the file
from google.colab import files
from langchain.document_loaders import PyPDFLoader
#Acces to choosing a file
uploaded=files.upload()
#Acces to store a file in variable and also store into list
pdf_path=list(uploaded.keys())[0]
loaders=PyPDFLoader("/content/"+ pdf_path)
docs=loaders.load()

#splitting the file into chunks
from langchain.text_splitter import RecursiveCharacterTextSplitter
#Assigning the chunks size
text_splitter=RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=100
)
chunks=text_splitter.split_documents(docs)

from langchain_google_genai import GoogleGenerativeAIEmbeddings
#Embedding the chunksor docs to make the it in the vector form
embeddings=GoogleGenerativeAIEmbeddings(model="models/embedding-001",google_api_key="AIzaSyAaI6cEtck9zu9Vb0UphPTez2BkFRzFXdw")

pip install faiss-cpu --quiet

from langchain_community.vectorstores import FAISS
#here we are using FAISS instead of qdrant vector for storing our all data
vectorstores=FAISS.from_documents(chunks,embeddings)
vectorstores.save_local("faiss_index")

# Commented out IPython magic to ensure Python compatibility.
# #Q/A Chain Retrieval Process
# #this is showing that the lining we are writing below this line stored in a file which we are makin called chtbot
# %%writefile chatbot.py
# #Now Giving the title name to chatbot
# import streamlit as st
# st.title("Chatbot")
# st.subheader("Ask your question from the uploaded book")
# #Giving permission to chatbot for taking input fron user
# query = st.text_input("Ask something")
# #Again we are importing and doing all these things bcz now we are entered into chatbot and now our every process is for inside chatbot
# #Our previous work is only for loading our data and working further on it.
# if query:
#     #Loding data from FAISS
#     from langchain_community.vectorstores import FAISS
#     from langchain_google_genai import GoogleGenerativeAIEmbeddings,ChatGoogleGenerativeAI
#     #Checking the related answer from the embedding chunks
#     embeddings=GoogleGenerativeAIEmbeddings(
#         model="models/embedding-001",google_api_key="AIzaSyAaI6cEtck9zu9Vb0UphPTez2BkFRzFXdw"
#     )
#     vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
#     llm=ChatGoogleGenerativeAI(
#         model="gemini-1.5-flash",
#         google_api_key="AIzaSyAaI6cEtck9zu9Vb0UphPTez2BkFRzFXdw"
#     )
#     #using the chain to get the answer
#     #using for the output and showinh the answer on chatbot
#     from langchain.chains import RetrievalQA
#     retriever=vectorstore.as_retriever()
#     qa_chain=RetrievalQA.from_chain_type(llm=llm,retriever=retriever)
#     #Output
#     result=qa_chain.run(query)
#     st.write(result)

#Run your chatbot
!streamlit run chatbot.py &> /content/logs.txt &

from pyngrok import ngrok
from pyngrok import conf, ngrok

conf.get_default().auth_token = "2zfunESQOXuU1fVAm7qWOen54Rw_88ysqJqm4T7Vogtmn1eLN"
!streamlit run chatbot.py &>/dev/null &
public_url = ngrok.connect(8501)
print(" Your chatbot is live at:", public_url)

